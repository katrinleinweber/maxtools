#!/usr/bin/env python

import logging, sys, optparse
import urllib
import urllib2
import xml.etree.ElementTree as et
import time

# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("usage: %prog [options] accList - download accesssions via eutils, accepts thousands of IDs")

parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages") 
parser.add_option("-o", "--oneByOne", dest="oneByOne", action="store_true", help="do not try to batch searches but rather search accessions one by one. This is necessary is you're looking for accessions with version numbers (e.g. NM_000325.5).")
parser.add_option("-f", "--format", dest="format", action="store", help="output format, default %default, can be fasta, gb or xml")
parser.add_option("", "--db", dest="db", action="store", help="NCBI database to query, default is %default", default="nuccore")
parser.add_option("", "--field", dest="field", action="store", type="int", help="only output field X (zero based) for the fasta ID line")
#parser.add_option("-f", "--file", dest="file", action="store", help="run on file") 
#parser.add_option("", "--test", dest="test", action="store_true", help="do something") 
(options, args) = parser.parse_args()

if options.debug:
    logging.basicConfig(level=logging.DEBUG)
else:
    logging.basicConfig(level=logging.INFO)

# ==== FUNCTIONs =====
BASEURL = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils"
MAXRATE = 3 # number of requests per second
DELAY   = 1/MAXRATE

database = "nuccore"
field = None

def chunkedDownloadFromEutils(accs, outFh, format="fasta"):
    " download in chunks of 5000 accs from eutils "
    chunkMax = 5000
    for chunkStart in range(0, len(accs), chunkMax):
        logging.info("Chunkstart is %d" % chunkStart)
        downloadFromEutils(accs[chunkStart:chunkStart+chunkMax], outFh, retType=format)

def eSearch(query):
    " run an esearch and return list of GIs "
    url = BASEURL+'/esearch.fcgi';
    params = {"db":database, "term":query}
    logging.debug("Running esearch with %s" % params)
    data = urllib.urlencode(params)
    req  = urllib2.Request(url, data)
    try:
        xmlStr = urllib2.urlopen(req).read()
    except urllib2.HTTPError:
        logging.error("Error when searching")
        raise
    logging.debug("XML reply: %s" % xmlStr)
    # parse xml
    root = et.fromstring(xmlStr)

    # parse from XML list of GIs
    try:
        gis = []
        count = int(root.find("Count").text)
        idEls = root.findall("IdList/Id")
        for idEl in idEls:
            gis.append(idEl.text)
    except AttributeError:
        logging.info("Error, XML is %s" % xmlStr)
        raise
    logging.debug("Got GIs: %s" % gis)
    wait(DELAY)
    return gis

def accsToGis_oneByOne(accs):
    """ if accs are old/supressed or with version number at the end, need to use this, a lot slower 
    
    """
    logging.info("Getting GIs for %d accessions, rate %d req/sec, one dot = 50 requests" % (len(accs), MAXRATE))
    gis = []
    count = 0
    for acc in accs:
        oneGi = eSearch(acc)
        assert(len(oneGi)==1)
        gis.extend(oneGi)
        if count%50==0:
            sys.stdout.write(".")
            sys.stdout.flush()
        count += 1
    return gis

def accsToGis(accs):
    """ search for accessions and post to eutils history server 
    return webenv, key, count
    """
    # query and put into eutils history
    logging.info("Running eutils search for %d accessions" % len(accs))
    accFull = [acc+"[accn]" for acc in accs]
    query = " OR ".join(accFull)
    query += ""
    gis = eSearch(query)
    return gis

lastCallSec = 0

def wait(delaySec):
    " make sure that delaySec seconds have passed between two calls, sleep if necessary "
    global lastCallSec
    delaySec = float(delaySec)
    nowSec = time.time()
    sinceLastCallSec = nowSec - lastCallSec
    if sinceLastCallSec > 0.01 and sinceLastCallSec < delaySec :
        waitSec = delaySec - sinceLastCallSec
        logging.debug("Waiting for %f seconds" % waitSec)
        time.sleep(waitSec)
    lastCallSec = time.time()

def eFetch(gis, outFh, retType="fasta", retMax=500):
    " download GIs "
    # fetch from history
    url = BASEURL + "/efetch.fcgi"
    logging.debug("URL: %s" % url)
    count = len(gis)
    retMode = "text"
    if retType=="xml":
        retMode = "xml"
        retType = "gb"
    for retStart in range(0, count, retMax):
        partGis = gis[retStart:retStart+retMax]
        logging.info("Retrieving %d records, start %d..." % (len(partGis), retStart))
        params = {"db":"nuccore", "id":",".join(partGis), "rettype":retType, "retmode" : retMode}
        data = urllib.urlencode(params)

        # issue http request
        logging.debug("HTTP post Data: %s" % data)
        req  = urllib2.Request(url, data)
        resp = urllib2.urlopen(req).read()
        if field!=None:
            resp = resp.strip()
            respLines = resp.splitlines()
            newLines = []
            for l in respLines:
                if l=="":
                    continue
                if l.startswith(">"):
                    newLines.append(">"+l.strip(">").split("|")[field])
                else:
                    newLines.append(l)
            resp = "\n".join(newLines)
        outFh.write(resp)
        wait(DELAY)
    
def downloadFromEutils(accs, outFh, retType="fasta", retMax=1000, oneByOne=False):
    " combine an esearch with efetch to download from eutils "
    if oneByOne:
        gis = accsToGis_oneByOne(accs)
    else:
        gis = accsToGis(accs)
    eFetch(gis, outFh, retType, retMax)

# ----------- MAIN --------------
if args==[]:
    parser.print_help()
    exit(1)

inFname, outFname = args
outFile = open(outFname, "w")

database = options.db
field = options.field
accs = open(inFname).read().splitlines()
downloadFromEutils(accs, outFile, oneByOne=options.oneByOne, retType=options.format)
